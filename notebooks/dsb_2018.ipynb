{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7bd4602-2de1-4c8b-9f37-37b66c559d73",
   "metadata": {},
   "source": [
    "# Image segmentation using U-Nets (Data Science Bowl 2018)\n",
    "\n",
    "### Segmentation of nuclei in microscopy images\n",
    "The segmentation of nuclei in microscopy images is a common task in many workflows. Most traditional image processing methods are created for one specific type of image, and perform poorly on others.\n",
    "\n",
    "The aim is to develop a general-purpose algorithm to segment cell nuclei from any type of microscopy images.\n",
    "\n",
    "<img src=\"assets/nuclei_image_types.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "---\n",
    "\n",
    "We will use a U-Net Convolutional Neural Network to perform this task. This notebook is adapted from [Keras U-Net starter by Kjetil Åmdal-Sævik](https://www.kaggle.com/keegil/keras-u-net-starter-lb-0-277?scriptVersionId=2164855) ([Licensed under the Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0))\n",
    "\n",
    "\n",
    "**For more information, see the original publication about the 2018 Data Science Bowl:**\n",
    "\n",
    "[Caicedo, J.C., Goodman, A., Karhohs, K.W. et al.\n",
    "Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl.\n",
    "Nat Methods 16, 1247–1253 (2019)](https://doi.org/10.1038/s41592-019-0612-7)\n",
    "\n",
    "We will be using some data from the original challenge (hosted at [https://www.kaggle.com/c/data-science-bowl-2018](https://www.kaggle.com/c/data-science-bowl-2018) (account required), and also from the [Broad Institute](https://bbbc.broadinstitute.org/BBBC038)).\n",
    "\n",
    "### **A copy of the data for this course is available [here](https://drive.google.com/file/d/1hlDAvRFH7Ax_Nxee4jCyN_IIDax-CLc8/view?usp=sharing)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9809620-ae49-497e-9c77-e47ca7d56777",
   "metadata": {},
   "source": [
    "## We begin by loading and visualising some of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd955c96-9433-474b-8349-ea5348dd7bd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the data\n",
    "We're using [pooch](https://www.fatiando.org/pooch/) to automate downloading the data, and check the hash."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054ad9d9-bdb0-4c53-8d1f-136bcaba6202",
   "metadata": {},
   "source": [
    "**Run the following cell to download the data in advance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e62cd71-33f6-4957-b008-d76ffcc9f7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pooch\n",
    "from pathlib import Path\n",
    "data_path = Path.cwd().parent / \"dsb_data\"\n",
    "\n",
    "# Use pooch to fetch data if it hasn't already been downloaded\n",
    "dsb_data_url = \"https://gin.g-node.org/neuroinformatics/image-analysis-courses/raw/master/dsb_2018/stage1_train.zip\"\n",
    "data_path = pooch.retrieve(dsb_data_url, known_hash=\"4e9fb804d2ff054d7bd83d4781c66e07dae9937207a09168330a5b5cefa0daba\", progressbar=True, processor=pooch.Unzip(extract_dir=data_path))\n",
    "data_path = Path(data_path[0]).parent.parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc69e38-9f9e-442f-a32a-9a90076f018d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "DEBUG = True  # Set to True to only load a fraction of the raw data\n",
    "\n",
    "\n",
    "def load_images(\n",
    "    data_path, image_width=128, image_height=128, num_channels=3, debug=False\n",
    "):\n",
    "    \"\"\"\n",
    "    :param data_path: Location of the data on disk (pathlib object)\n",
    "    :param filenames: List of image paths\n",
    "    :param image_width: Image width in pixels for resizing\n",
    "    :param image_height: Image height in pixels for resizing\n",
    "    :param num_channels: Final number of channels required\n",
    "\n",
    "    :return images, masks: Numpy array of images, and segmentation masks\n",
    "    \"\"\"\n",
    "\n",
    "    data_path = Path(data_path)\n",
    "\n",
    "    filenames = next(os.walk(data_path))[1]\n",
    "    if debug:\n",
    "        filenames = filenames[:40]\n",
    "        \n",
    "    images = np.zeros(\n",
    "        (len(filenames), image_height, image_width, num_channels),\n",
    "        dtype=np.uint8,\n",
    "    )\n",
    "    masks = np.zeros((len(filenames), image_height, image_width, 1), dtype=bool)\n",
    "\n",
    "    for n, image_id in tqdm(enumerate(filenames), total=len(filenames)):\n",
    "        img = imread(data_path / image_id / \"images\" / (image_id + \".png\"))[\n",
    "            :, :, :num_channels\n",
    "        ]\n",
    "        img = resize(\n",
    "            img,\n",
    "            (image_height, image_width),\n",
    "            mode=\"constant\",\n",
    "            preserve_range=True,\n",
    "        )\n",
    "        images[n] = img\n",
    "        mask = np.zeros((image_height, image_width, 1), dtype=bool)\n",
    "\n",
    "        mask_dir = data_path / image_id / \"masks\"\n",
    "        for mask_file in next(os.walk(mask_dir))[2]:\n",
    "            mask_ = imread(mask_dir / mask_file)\n",
    "            mask_ = np.expand_dims(\n",
    "                resize(\n",
    "                    mask_,\n",
    "                    (image_height, image_width),\n",
    "                    mode=\"constant\",\n",
    "                    preserve_range=True,\n",
    "                ),\n",
    "                axis=-1,\n",
    "            )\n",
    "            mask = np.maximum(mask, mask_)\n",
    "        masks[n] = mask\n",
    "    return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8732ea0-d918-4760-9c28-5a64b30290e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images, masks = load_images(data_path, debug=DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb24e6-5b80-4b85-8607-daf1f856064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise some raw data and segmentation masks\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "\n",
    "def plot_raw_data(raw_images, masks, num_to_plot=2, fig_size=(10, 10), axes_pad=0.1):\n",
    "    \"\"\"\n",
    "    Plot raw data\n",
    "\n",
    "    :param raw_images: Array of raw data images\n",
    "    :param masks: Array of mask images\n",
    "    :param num_to_plot: How many random images to plot\n",
    "    :param fig_size: Tuple of overall figure size\n",
    "    :param axes_pad: Seperation of individual images\n",
    "    \"\"\"\n",
    "\n",
    "    idx_to_plot = tuple(random.sample(range(0, len(raw_images)), num_to_plot))\n",
    "\n",
    "    raw_images = raw_images[tuple([idx_to_plot])]\n",
    "    masks = np.squeeze(masks)[tuple([idx_to_plot])]\n",
    "\n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    grid = ImageGrid(\n",
    "        fig,\n",
    "        111,\n",
    "        nrows_ncols=(num_to_plot, 2),\n",
    "        axes_pad=axes_pad,\n",
    "    )\n",
    "\n",
    "    images = []\n",
    "    for a, b in zip(raw_images, masks):\n",
    "        images.extend([a, b])\n",
    "\n",
    "    for ax, im in zip(grid, images):\n",
    "        ax.imshow(im)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    grid[0].title.set_text(\"Raw data\")\n",
    "    grid[1].title.set_text(\"Segmentation masks\")\n",
    "\n",
    "\n",
    "plot_raw_data(images, masks, num_to_plot=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159612b0-d841-484f-b35e-aa560a502573",
   "metadata": {},
   "source": [
    "## Now we'll build a model to segment these images\n",
    "\n",
    "We'll be using the popular [U-Net architecture](https://arxiv.org/abs/1505.04597)\n",
    "\n",
    "<img src=\"assets/unet.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea23f970-d733-4596-8085-0f21d47be7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 15:41:16.133382: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-01 15:41:16.241584: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-01 15:41:16.764622: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-01 15:41:16.764651: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-01 15:41:16.764654: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# This will import TensorFlow, which will likely print lots of warnings to the user.\n",
    "# These can generally be safely ignored. \n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import concatenate\n",
    "\n",
    "\n",
    "def build_unet(image_width=128, image_height=128, num_channels=3):\n",
    "    inputs = Input((image_height, image_width, num_channels))\n",
    "    s = Lambda(lambda x: x / 255)(inputs)\n",
    "\n",
    "    c1 = Conv2D(8, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    c1 = Conv2D(8, (3, 3), activation=\"relu\", padding=\"same\")(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\")(p1)\n",
    "    c2 = Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\")(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(p2)\n",
    "    c3 = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(p3)\n",
    "    c4 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(c4)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "\n",
    "    c5 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(p4)\n",
    "    c5 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(c5)\n",
    "\n",
    "    u6 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding=\"same\")(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(u6)\n",
    "    c6 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(c6)\n",
    "\n",
    "    u7 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding=\"same\")(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(u7)\n",
    "    c7 = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(c7)\n",
    "\n",
    "    u8 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding=\"same\")(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\")(u8)\n",
    "    c8 = Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\")(c8)\n",
    "\n",
    "    u9 = Conv2DTranspose(8, (2, 2), strides=(2, 2), padding=\"same\")(c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = Conv2D(8, (3, 3), activation=\"relu\", padding=\"same\")(u9)\n",
    "    c9 = Conv2D(8, (3, 3), activation=\"relu\", padding=\"same\")(c9)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation=\"sigmoid\")(c9)\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b731ff-8808-4d3f-97d4-0bbfbcc044c0",
   "metadata": {},
   "source": [
    "**N.B. For most applications, you wouldn't need to build your own network architecture. Most common archictures are available in open-source packages, including [in keras itself](https://keras.io/api/applications/)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a62aea9-bbd6-4911-95a9-b3b019f3a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = build_unet()\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(0.0001),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.MeanIoU(num_classes=2),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tf.keras.metrics.Precision(),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9734b1-9c91-47e2-98c6-c2bd5509f5b8",
   "metadata": {},
   "source": [
    "## Split the dataset into a train (80%) and test (20%) fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311cfc12-4d12-4ad2-ab54-cd50d974f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_fraction = 0.2\n",
    "\n",
    "(\n",
    "    images_train,\n",
    "    images_test,\n",
    "    masks_train,\n",
    "    masks_test,\n",
    ") = train_test_split(\n",
    "    images,\n",
    "    masks,\n",
    "    test_size=test_fraction,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4d9fb8-f122-4635-bb88-b863f9795509",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2889fef7-6f37-4471-bc60-4968c90dc19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100  # Number of iterations over the full dataset\n",
    "batch_size = 32  # Number of samples per gradient update\n",
    "validation_split = 0.1  # In addition to test dataset, use 10% of data to assess performance during training\n",
    "\n",
    "model.fit(\n",
    "    images_train,\n",
    "    masks_train,\n",
    "    validation_split=validation_split,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8112f9ed-7f0c-4de4-88ae-63d673fc0f0a",
   "metadata": {},
   "source": [
    "## Apply network to (unseen) test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286e8526-42b2-42bb-86e9-fc8d2c8750eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_masks = model.predict(images_test, verbose=1)\n",
    "predicted_masks = predicted_masks > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e8ef7-d36f-4e51-903f-6354875b37b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise some results\n",
    "def plot_predictions(\n",
    "    raw_images,\n",
    "    predictions,\n",
    "    ground_truth,\n",
    "    num_to_plot=2,\n",
    "    fig_size=(15, 15),\n",
    "    axes_pad=0.1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare predicted & actual mask images\n",
    "\n",
    "    :param raw_images: Array of raw data images\n",
    "    :param predictions: Array of prediction mask images\n",
    "    :param ground_truth: Array of ground truth mask images\n",
    "    :param num_to_plot: How many random images to plot\n",
    "    :param fig_size: Tuple of overall figure size\n",
    "    :param axes_pad: Seperation of individual images\n",
    "    \"\"\"\n",
    "\n",
    "    idx_to_plot = tuple(random.sample(range(0, len(raw_images)), num_to_plot))\n",
    "\n",
    "    raw_images = raw_images[tuple([idx_to_plot])]\n",
    "    predictions = np.squeeze(predictions)[tuple([idx_to_plot])]\n",
    "    ground_truth = np.squeeze(ground_truth)[tuple([idx_to_plot])]\n",
    "\n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    grid = ImageGrid(\n",
    "        fig,\n",
    "        111,\n",
    "        nrows_ncols=(num_to_plot, 3),\n",
    "        axes_pad=axes_pad,\n",
    "    )\n",
    "\n",
    "    images = []\n",
    "    for a, b, c in zip(raw_images, predictions, ground_truth):\n",
    "        images.extend([a, b, c])\n",
    "\n",
    "    for ax, im in zip(grid, images):\n",
    "        ax.imshow(im)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    grid[0].title.set_text(\"Raw data\")\n",
    "    grid[1].title.set_text(\"Network prediction\")\n",
    "    grid[2].title.set_text(\"Ground truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d5a4c-ec13-4c9c-9c10-f490c74868e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(images_test, predicted_masks, masks_test, num_to_plot=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d69bbc-4bd0-46cc-9343-7806fb7760c2",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "* Segmentation not perfect, lots of ways to improve (more training, augmentation, better network, adjust hyperparameters etc.)\n",
    "* We segmented *all* cells, and not *each* cell. We could further these masks (e.g. by using a watershed algorithm), or use an alternative, object detection network (e.g. [Mask R-CNN](https://arxiv.org/abs/1703.06870))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:image-analysis] *",
   "language": "python",
   "name": "conda-env-image-analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
